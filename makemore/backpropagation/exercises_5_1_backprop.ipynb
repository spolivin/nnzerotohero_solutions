{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "# Preraring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to the exercise, we will have to conduct the exact same data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's include all imports of libraries we want in a single cell for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TORCH_GEN_SEED = 2147483647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to load the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening the dataset with names and reading its content into a variable\n",
    "words = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary and creating character-number mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the vocabulary as well as mappings from character to its identifier and backwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character -> Identifier:\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "\n",
      "Identifier -> Character:\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "\n",
      "Vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# Retrieving a set of unique letters\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# Creating a mapping from a letter to an id\n",
    "char2id = {s: i+1 for i, s in enumerate(chars)}\n",
    "# Adding the start_of_word/end_of_word token => \".\"\n",
    "char2id['.'] = 0\n",
    "\n",
    "# Creating a mapping from an id to letter\n",
    "id2char = {i: s for s, i in char2id.items()}\n",
    "\n",
    "# Computing the size of the vocabulary\n",
    "vocab_size = len(id2char)\n",
    "\n",
    "# Displaying the mappings and vocab size\n",
    "print(\"Character -> Identifier:\")\n",
    "print(char2id)\n",
    "print()\n",
    "print(\"Identifier -> Character:\")\n",
    "print(id2char)\n",
    "print()\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 0-25,625\n",
      "Development set: 25,626-28,828\n",
      "Testing set: 28,829-32,032\n",
      "\n",
      "Total words: 32,033\n"
     ]
    }
   ],
   "source": [
    "# Setting the random seed and reshuffling names\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(words)\n",
    "\n",
    "# Determining cutoff points for 10% dev and 10% test\n",
    "cutoff_train = int(0.8*len(words))\n",
    "cutoff_dev = int(0.9*len(words))\n",
    "\n",
    "print(f\"Training set: {0}-{cutoff_train-1:,}\")\n",
    "print(f\"Development set: {cutoff_train:,}-{cutoff_dev-1:,}\")\n",
    "print(f\"Testing set: {cutoff_dev:,}-{len(words)-1:,}\")\n",
    "\n",
    "print(f\"\\nTotal words: {len(words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set examples: 25,626 (80%)\n",
      "Development set examples: 3,203 (10%)\n",
      "Testing set examples: 3,204 (10%)\n"
     ]
    }
   ],
   "source": [
    "# Allocating shuffled words into three sets\n",
    "words_train = words[:cutoff_train]\n",
    "words_dev = words[cutoff_train:cutoff_dev]\n",
    "words_test = words[cutoff_dev:]\n",
    "\n",
    "print(f\"Training set examples: {len(words_train):,} ({len(words_train)/len(words):.0%})\")\n",
    "print(f\"Development set examples: {len(words_dev):,} ({len(words_dev)/len(words):.0%})\")\n",
    "print(f\"Testing set examples: {len(words_test):,} ({len(words_test)/len(words):.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, block_size):\n",
    "    \"\"\"Creates a dataset of encoded characters.\"\"\"\n",
    "    # Preallocating lists for dataset\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        # Creating a starting examples depending on block size\n",
    "        context = [0] * block_size\n",
    "        # Iterating through entire word with end of word token\n",
    "        for char in word + '.':\n",
    "            index = char2id[char]\n",
    "            X.append(context)\n",
    "            Y.append(index)\n",
    "            # Adding the character index and shifting\n",
    "            context = context[1:] + [index]\n",
    "    \n",
    "    # Casting as PyTorch tensors\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the block size (number of character to use to predict the next one)\n",
    "block_size = 3\n",
    "\n",
    "# Building the datasets for three sets\n",
    "X_train, Y_train = build_dataset(\n",
    "    words=words_train, block_size=block_size,\n",
    ")\n",
    "X_dev, Y_dev = build_dataset(\n",
    "    words=words_dev, block_size=block_size,\n",
    ")\n",
    "X_test, Y_test = build_dataset(\n",
    "    words=words_test, block_size=block_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-epoch forward/backward pass runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run the training loop for one iteration in order to be able to collect the gradients computed using `backward` method from `torch` that will be later used to doing gradient comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first obvious step is to initialize all weights of the neural net. In the following cell we fix: \n",
    "\n",
    "* dimensionality of the space into which the characters fed into the net will be mapped (`n_embd`);\n",
    "* number of neurons of which the hidden layer will be composed (`n_hidden`);\n",
    "* random number generator (`generator`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the character embedding vectors\n",
    "n_embd = 10\n",
    "\n",
    "# number of neurons in the hidden layer of the MLP\n",
    "n_hidden = 64\n",
    "\n",
    "# Setting up the random numbers generator (for reproducibility)\n",
    "generator = torch.Generator().manual_seed(TORCH_GEN_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the MLP weights themselves. It should be noted that the parameters are initialized in such a way that the comparison of the backward pass implementations becomes more accurate (initialization by zeros can mask the incorrect backpropagation implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [],
   "source": [
    "# Initializing the character embedding matrix\n",
    "C  = torch.randn((vocab_size, n_embd), generator=generator)\n",
    "\n",
    "# LAYER 1 WEIGHTS (Kaiming initialization)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=generator)\n",
    "W1 *= (5/3) / ((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=generator) * 0.1\n",
    "\n",
    "# LAYER 2 WEIGHTS\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=generator) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=generator) * 0.1\n",
    "\n",
    "# BATCHNORM LAYER PARAMETERS\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine all parameters to be trained together, compute its total number and enable gradient computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 4,137\n"
     ]
    }
   ],
   "source": [
    "# Putting all trainable parameters together\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "# Computing the number of parameters\n",
    "num_params = sum(p.nelement() for p in parameters)\n",
    "print(f\"Number of trainable parameters: {num_params:,}\") # number of parameters in total\n",
    "\n",
    "# Making all parameters trainable (enabling gradient computation)\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [],
   "source": [
    "# Setting batch size\n",
    "batch_size = 32\n",
    "n = batch_size\n",
    "\n",
    "# Constructing a minibatch\n",
    "ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=generator)\n",
    "Xb, Yb = X_train[ix], Y_train[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8ofj1s6d8PPv"
   },
   "outputs": [],
   "source": [
    "# Embedding the characters into vectors\n",
    "emb = C[Xb]\n",
    "# Concatenating the vectors\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "#################################################\n",
    "\n",
    "# LINEAR LAYER 1\n",
    "\n",
    "# Hidden layer pre-activation\n",
    "hprebn = embcat @ W1 + b1\n",
    "\n",
    "#################################################\n",
    "\n",
    "# BATCHNORM LAYER\n",
    "\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "#################################################\n",
    "\n",
    "# TANH LAYER\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "#################################################\n",
    "\n",
    "# LINEAR LAYER 2\n",
    "\n",
    "# Output layer\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "#################################################\n",
    "\n",
    "# CROSS-ENTROPY LOSS\n",
    "\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3236, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeroing all gradients\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "    \n",
    "# Computing gradients for each element of the computational graph\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Manual backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have to manually backpropagate throught the net and compare the computed gradients with those computed by Pytorch. We will use `cmp` function for being able to make such comparison which is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    \"\"\"Compares manual gradients to Pytorch gradients.\"\"\"\n",
    "    # Returning True if all gradients are exactly equal\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    # Returning True if all gradients are approximately equal\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    # Computing the maximum difference between gradients\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    # Printing the comparison information\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go layer by layer backwards computing gradients and comparing them to those given by Pytorch:\n",
    "\n",
    "1. Cross-entropy layer\n",
    "2. Linear layer 2\n",
    "3. Tanh layer\n",
    "4. Linear layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dlogprobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element we have to backpropagate through starts at the end of the forward pass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we need to find the derivative of `loss` with respect to all of the elements of `logprobs`. Let's consider the shape of `logprobs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we basically do when computing the loss is that we just go across the rows of `logprobs` and pluck out a specific value from each row. `Yb` represents all the correct answers from the minibatch and we use them to select the elements of logprobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5977, -2.5081, -3.8608, -3.0493, -3.9454, -2.5466, -3.7662, -3.2970,\n",
       "         -3.8562, -3.4023, -3.2474, -3.3003, -3.2509, -3.5452, -3.3283, -4.2301,\n",
       "         -4.6508, -3.9057, -4.1790, -2.9146, -2.9798, -3.8614, -3.5935, -2.6896,\n",
       "         -2.9245, -3.5907, -3.8202],\n",
       "        [-2.8943, -2.9508, -2.3043, -3.0001, -3.3874, -3.4851, -4.0099, -3.1080,\n",
       "         -3.9075, -3.6021, -2.9709, -3.1078, -3.0784, -3.5860, -3.0310, -3.1503,\n",
       "         -3.6338, -3.9293, -3.9368, -3.3053, -3.9206, -3.8271, -4.2068, -2.7865,\n",
       "         -3.7794, -3.2778, -3.6353],\n",
       "        [-3.8616, -3.8408, -4.1239, -4.4752, -3.8493, -3.1846, -2.8557, -2.7162,\n",
       "         -2.8016, -3.4985, -3.8457, -3.3742, -3.1721, -3.0913, -3.7645, -3.5857,\n",
       "         -4.2611, -3.3480, -3.5715, -2.1515, -2.7222, -3.3030, -3.1284, -3.3161,\n",
       "         -3.4061, -3.8846, -3.5488],\n",
       "        [-3.3517, -3.7365, -3.0457, -3.0286, -2.8694, -3.6259, -3.0988, -3.1669,\n",
       "         -3.0560, -3.8834, -3.2423, -3.6165, -3.3994, -3.2630, -2.8299, -2.6186,\n",
       "         -3.7311, -3.6058, -4.1252, -2.9813, -3.9593, -3.8099, -3.1667, -3.8871,\n",
       "         -3.7171, -3.0441, -3.1336],\n",
       "        [-4.0211, -4.1877, -3.6922, -3.8281, -4.1103, -3.1688, -3.3587, -3.9110,\n",
       "         -2.8802, -3.2599, -3.3561, -4.1476, -3.2805, -3.7887, -3.1449, -1.7320,\n",
       "         -3.9894, -2.9479, -3.0149, -2.7903, -3.1364, -4.2437, -3.7845, -4.2207,\n",
       "         -3.3142, -3.7726, -3.0888],\n",
       "        [-3.3192, -3.3855, -2.7769, -2.8474, -2.7612, -3.7304, -3.4626, -2.9249,\n",
       "         -3.6830, -4.1194, -3.0942, -4.0075, -3.5859, -3.1769, -2.7353, -2.9016,\n",
       "         -4.0919, -3.7755, -4.5611, -3.4529, -3.4419, -3.5606, -3.1007, -4.0044,\n",
       "         -4.0756, -2.5498, -3.1491],\n",
       "        [-2.8315, -4.5429, -2.7218, -4.2143, -3.5953, -3.9944, -2.7047, -2.8288,\n",
       "         -2.9216, -3.1243, -3.9613, -3.2717, -3.6105, -4.0296, -4.1939, -2.6388,\n",
       "         -2.5990, -3.0838, -4.5934, -3.4294, -3.5946, -3.0311, -3.0861, -3.8524,\n",
       "         -3.3834, -3.7362, -3.5469],\n",
       "        [-3.0924, -3.4999, -3.6854, -2.2034, -3.6261, -2.9791, -3.4706, -2.4599,\n",
       "         -3.2378, -4.7633, -3.6245, -4.2254, -3.8884, -2.8585, -4.0221, -4.1773,\n",
       "         -4.2559, -4.2200, -4.2236, -2.3707, -3.3019, -3.0687, -3.5962, -3.3750,\n",
       "         -3.8169, -2.8855, -3.3511],\n",
       "        [-3.0109, -3.6341, -4.0570, -3.3337, -4.4903, -3.2409, -2.8749, -2.3346,\n",
       "         -3.5246, -3.8879, -3.5087, -3.4714, -3.2486, -3.6931, -4.5242, -3.9824,\n",
       "         -3.9184, -3.2316, -3.8518, -2.3434, -3.3250, -2.5475, -3.1397, -3.1030,\n",
       "         -3.2875, -4.1792, -3.4460],\n",
       "        [-3.5051, -4.3274, -3.3845, -4.0649, -3.5333, -3.3302, -3.3946, -3.4462,\n",
       "         -2.3938, -2.7808, -4.2387, -3.2380, -3.2831, -3.2286, -3.7057, -3.4575,\n",
       "         -2.8005, -3.0458, -3.2147, -3.6719, -2.6829, -3.7249, -2.7173, -3.9165,\n",
       "         -3.5832, -3.3350, -3.9456],\n",
       "        [-4.1286, -4.9519, -3.1933, -3.1856, -3.4670, -3.3541, -4.0217, -4.9593,\n",
       "         -3.3087, -3.0567, -3.6866, -3.9838, -2.9165, -3.5488, -3.1851, -1.9233,\n",
       "         -2.9676, -2.7864, -2.5622, -3.2482, -3.0983, -4.5697, -4.3755, -3.7769,\n",
       "         -3.4038, -3.5817, -3.7053],\n",
       "        [-3.7507, -3.5360, -3.6250, -1.7218, -4.3418, -2.9279, -2.6499, -3.0381,\n",
       "         -3.1276, -3.4409, -4.1806, -3.9502, -3.8517, -3.6101, -4.9771, -4.8140,\n",
       "         -3.7763, -3.2094, -4.1970, -2.8956, -2.8860, -3.3953, -3.9801, -3.2413,\n",
       "         -3.1592, -3.1329, -4.1110],\n",
       "        [-3.8908, -4.3446, -2.9365, -4.2839, -3.6820, -4.1105, -2.5478, -3.1624,\n",
       "         -2.8576, -3.0032, -4.0680, -3.2909, -3.2726, -3.5916, -4.1883, -3.1502,\n",
       "         -2.6191, -3.0567, -3.2473, -3.6827, -3.0440, -3.2202, -2.3304, -4.5265,\n",
       "         -3.9325, -3.6489, -3.5922],\n",
       "        [-3.6151, -3.6488, -2.8784, -3.9679, -4.0832, -2.6041, -3.6428, -4.1343,\n",
       "         -3.5446, -2.9799, -3.4224, -3.3547, -2.7819, -3.3200, -2.9392, -3.0473,\n",
       "         -3.5432, -3.3378, -2.6083, -4.2409, -2.7138, -4.2126, -4.0207, -3.3423,\n",
       "         -3.4725, -3.2413, -3.3195],\n",
       "        [-3.0286, -3.8393, -3.5025, -2.9774, -4.0847, -3.3966, -3.4979, -2.7546,\n",
       "         -2.8299, -3.0157, -3.4342, -3.1346, -2.9631, -3.2894, -3.6352, -3.6649,\n",
       "         -3.8411, -2.8669, -3.6585, -2.9792, -2.9007, -3.0345, -3.6215, -3.4287,\n",
       "         -4.0234, -3.8354, -3.7532],\n",
       "        [-3.1084, -2.9094, -3.3862, -4.0304, -3.9599, -2.5307, -3.5747, -3.6688,\n",
       "         -3.5079, -2.9029, -3.3173, -3.0286, -2.7805, -4.1120, -3.5236, -3.9818,\n",
       "         -3.2037, -3.8611, -2.9120, -4.0198, -3.2146, -3.7117, -3.3993, -2.7164,\n",
       "         -2.9754, -3.5186, -3.8045],\n",
       "        [-3.5131, -3.8205, -2.4017, -3.2141, -2.7949, -3.0095, -4.1547, -3.5873,\n",
       "         -3.9699, -3.8948, -2.9543, -3.8263, -3.1991, -2.9390, -2.7019, -2.6782,\n",
       "         -4.2188, -3.9568, -3.8505, -3.1278, -3.5858, -4.2147, -4.6896, -3.0824,\n",
       "         -3.8540, -3.1060, -2.8357],\n",
       "        [-2.9020, -2.5854, -3.0017, -2.9260, -3.2740, -3.4472, -3.8197, -2.6324,\n",
       "         -3.6691, -3.9951, -2.6586, -4.3044, -3.7711, -3.9290, -3.1101, -3.5105,\n",
       "         -4.2139, -3.4208, -4.1383, -2.9290, -3.8877, -3.0711, -3.5673, -3.9620,\n",
       "         -3.7334, -3.1321, -2.8181],\n",
       "        [-3.8908, -4.3446, -2.9365, -4.2839, -3.6820, -4.1105, -2.5478, -3.1624,\n",
       "         -2.8576, -3.0032, -4.0680, -3.2909, -3.2726, -3.5916, -4.1883, -3.1502,\n",
       "         -2.6191, -3.0567, -3.2473, -3.6827, -3.0440, -3.2202, -2.3304, -4.5265,\n",
       "         -3.9325, -3.6489, -3.5922],\n",
       "        [-3.7056, -3.9425, -3.2910, -4.1828, -3.3634, -3.4044, -2.6463, -2.8771,\n",
       "         -3.3248, -3.3758, -3.4946, -3.2034, -2.8699, -2.9607, -4.2393, -3.6447,\n",
       "         -3.5239, -3.8720, -3.5522, -2.4362, -2.9877, -3.3677, -3.1447, -3.0875,\n",
       "         -3.5521, -3.7754, -3.6239],\n",
       "        [-2.8995, -4.3401, -3.9722, -3.3875, -3.5358, -3.3342, -3.5394, -3.1693,\n",
       "         -2.8945, -3.8147, -2.7374, -3.3383, -2.8409, -3.9155, -3.1723, -2.4757,\n",
       "         -4.0822, -3.7935, -3.3337, -2.8671, -3.9970, -3.5872, -3.6358, -3.7193,\n",
       "         -3.3665, -2.8667, -3.1808],\n",
       "        [-2.9059, -2.7178, -2.8837, -3.1215, -3.6757, -2.9317, -3.5741, -3.0500,\n",
       "         -3.7828, -3.9668, -2.8731, -3.5593, -3.7032, -3.5160, -3.1922, -2.9504,\n",
       "         -3.9673, -3.1380, -3.9431, -2.4918, -4.2599, -3.5012, -4.1901, -3.5420,\n",
       "         -3.4875, -3.5123, -3.2820],\n",
       "        [-3.8908, -4.3446, -2.9365, -4.2839, -3.6820, -4.1105, -2.5478, -3.1624,\n",
       "         -2.8576, -3.0032, -4.0680, -3.2909, -3.2726, -3.5916, -4.1883, -3.1502,\n",
       "         -2.6191, -3.0567, -3.2473, -3.6827, -3.0440, -3.2202, -2.3304, -4.5265,\n",
       "         -3.9325, -3.6489, -3.5922],\n",
       "        [-2.9716, -4.0570, -3.5308, -4.4937, -2.9360, -3.7149, -2.7917, -3.3567,\n",
       "         -3.6799, -3.1404, -3.3960, -3.1459, -3.3792, -3.4491, -3.2940, -2.9552,\n",
       "         -2.6557, -3.7646, -3.3870, -4.1760, -3.8935, -3.8655, -2.1680, -3.9654,\n",
       "         -2.9953, -3.4579, -3.9868],\n",
       "        [-3.3168, -2.5341, -3.7169, -3.0906, -3.6824, -2.7841, -3.7607, -3.5202,\n",
       "         -3.5880, -3.4107, -2.3064, -4.0266, -3.5072, -4.4523, -3.2610, -3.8820,\n",
       "         -4.0815, -4.0231, -4.4525, -3.6009, -4.2591, -3.7652, -4.4896, -1.9231,\n",
       "         -3.0253, -3.5793, -2.9711],\n",
       "        [-2.7115, -3.6374, -4.1315, -4.0225, -3.5577, -2.5218, -3.1691, -3.3795,\n",
       "         -3.5355, -3.3613, -3.4466, -2.8770, -2.9125, -3.3903, -3.8360, -4.2771,\n",
       "         -3.4189, -3.8595, -2.9412, -4.2511, -3.5991, -3.8409, -2.5997, -2.9943,\n",
       "         -2.7827, -3.1019, -4.4274],\n",
       "        [-3.0967, -3.7522, -3.7727, -4.6511, -3.2115, -3.9271, -2.4935, -3.1022,\n",
       "         -3.4596, -2.9492, -3.9976, -2.9951, -2.8263, -3.5278, -3.6784, -3.9208,\n",
       "         -2.9260, -3.5986, -3.6187, -3.4448, -3.7512, -3.3536, -1.9903, -3.9889,\n",
       "         -3.4053, -3.5789, -4.5188],\n",
       "        [-3.5654, -3.6191, -2.9248, -3.9462, -3.4570, -3.7567, -3.3250, -3.4679,\n",
       "         -3.1631, -2.9614, -3.7878, -3.5413, -3.2748, -3.3761, -3.0945, -2.6926,\n",
       "         -3.1131, -3.0398, -3.0257, -4.0433, -2.4059, -4.1944, -3.0183, -4.4307,\n",
       "         -4.1697, -3.3400, -3.1263],\n",
       "        [-2.6446, -3.3519, -4.0797, -3.9957, -3.3645, -2.8478, -3.1530, -3.1177,\n",
       "         -4.5576, -3.5982, -3.0581, -3.4454, -3.7029, -3.8380, -3.5745, -3.3666,\n",
       "         -2.5457, -3.6246, -2.8258, -3.7603, -3.3902, -3.5555, -3.0374, -2.8835,\n",
       "         -2.6918, -4.4242, -3.9054],\n",
       "        [-3.7448, -4.1361, -3.3100, -3.5631, -3.0294, -3.8990, -2.7439, -3.3566,\n",
       "         -2.6450, -3.3978, -3.5097, -3.3061, -3.5023, -3.3864, -3.4944, -2.5831,\n",
       "         -3.5453, -3.5161, -3.3684, -3.4871, -3.0918, -3.7032, -2.8379, -4.2217,\n",
       "         -3.5085, -2.9531, -3.2745],\n",
       "        [-3.1882, -3.0669, -3.6389, -2.8418, -3.0988, -3.1025, -3.3083, -3.3828,\n",
       "         -3.5441, -3.8170, -3.2552, -3.7409, -4.0175, -3.6774, -4.5217, -3.2450,\n",
       "         -3.3538, -3.4239, -3.1953, -2.9762, -2.8839, -3.6514, -3.7662, -2.6203,\n",
       "         -2.5891, -3.8711, -3.5931],\n",
       "        [-3.0790, -3.8345, -2.5936, -4.0285, -2.6453, -3.5297, -4.0627, -3.6202,\n",
       "         -4.1412, -2.9226, -3.6120, -3.4479, -3.4029, -2.6026, -3.2096, -3.2050,\n",
       "         -2.9573, -3.2012, -3.1773, -4.2897, -2.8039, -4.5971, -2.8060, -3.7883,\n",
       "         -4.0300, -3.2572, -3.8791]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
       "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.8562, -3.0310, -3.5857, -3.1667, -4.0211, -3.4529, -3.1243, -4.0221,\n",
       "        -3.2409, -4.3274, -3.0983, -1.7218, -2.8576, -2.9392, -2.9631, -3.1084,\n",
       "        -3.8263, -2.9020, -3.5922, -3.3758, -2.8667, -2.9059, -4.3446, -4.0570,\n",
       "        -3.5202, -2.9412, -2.9492, -3.9462, -2.8478, -3.3978, -3.1882, -3.1773],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3236, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-logprobs[range(n), Yb].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to define what the derivative of this function would look like, we can look at a simpler example. Let's rewrite the loss function in a much simpler form where we have just 3 log-probabilities instead of 32:\n",
    "\n",
    "$$loss = -\\frac{a + b + c}{3}$$\n",
    "\n",
    "Now the derivatives:\n",
    "\n",
    "$$\\frac{\\partial{loss}}{\\partial{a}} = -\\frac{1}{3}$$\n",
    "\n",
    "$$\\frac{\\partial{loss}}{\\partial{b}} = -\\frac{1}{3}$$\n",
    "\n",
    "$$\\frac{\\partial{loss}}{\\partial{c}} = -\\frac{1}{3}$$\n",
    "\n",
    "In other words, if we interpolate the results of the above equations to our case, then basically, given that we have `n` instead of `3`, the derivative will take the value of $-\\frac{1}{n}$ in all the places that have been plucked out. As far as the other elements are concerned, gradients for these will be zeros, since these simply do not participate in the loss calculation. Hence, changing these elements will not change the loss function in any way so their respective derivatives are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dlogprobs\"\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dprobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go one step back and this is the equation we need to backpropagate through:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "logprobs = probs.log()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape, probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes are the equal to each other so we can just take a simple derivative from calculus (not forgetting the chain rule):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dprobs\"\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dcounts_sum_inv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After considering the shapes above, we see that the dimensions are different for the elements of the multiplication. When encountering such situations, `torch` implements a *broadcasting*: for instance, here `counts_sum_inv` will get replicated across the columns of `counts` 27 times, hence enabling element-wise multiplication. \n",
    "\n",
    "We can demonstrate it in a simpler example. Suppose we also have 3 matrices that are connected like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C = A b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices on the right-hand side look as follows (take for instance 3 dimensions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$b = \\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "b_{3}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, applying broadcasting will result in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C = \\begin{bmatrix}\n",
    "a_{11} b_1 & a_{12} b_1 & a_{13} b_1\\\\\n",
    "a_{21} b_2 & a_{22} b_2 & a_{23} b_2\\\\\n",
    "a_{31} b_3 & a_{32} b_3 & a_{33} b_3\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically replicated $b$ vector across the columns of $A$. Taking $\\frac{\\partial{probs}}{\\partial{counts\\_sum\\_inv}}$ results in basically taking elements of `count` (multiplied by `dprobs` from the chain rule) and summing them up across the columns (since for instance $b_1$ occurs in all columns and these contributions should be accumulated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dcounts_sum_inv\"\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dcounts`, `dcounts_sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding `dcounts` from the code below can be tricky, since `counts` occurs in two different nodes and as a result such contributions should be summed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dcounts` (first node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will consider the first branch where `counts` occurs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "probs = counts * counts_sum_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape, counts.shape, counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the result of the calculations above we can just multiply `counts_sum_inv` by the `dprobs` from the chain rule to obtain the matrix with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: False | approximate: False | maxdiff: 0.005585933104157448\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dcounts\" (Node 1)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the computed gradient is actually incorrect, since we have only considered one node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dcounts_sum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "counts_sum_inv = counts_sum**-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.shape, counts_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes are indentical so there is nothing difficult to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dcounts_sum\"\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dcounts` (second node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go on to consider the second node through which `counts` flows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum.shape, counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcounts_sum.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simpler example and depict what this line of code achieves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$A^{sum} = \\begin{bmatrix}\n",
    "a_{11} + a_{12} + a_{13}\\\\\n",
    "a_{21} + a_{22} + a_{23}\\\\\n",
    "a_{31} + a_{32} + a_{33}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically take the elements of `counts` and sum the elements across each column. Hence, when taking the derivative across each parameter we will get a matrix of ones to be multiplied with `dcounts_sum` from the chain rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Accumulating \"dloss/dcounts\" from Node 2\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dnorm_logits`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "counts = norm_logits.exp()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape, norm_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes are the same so we can just take a simple derivative. Since the derivative of the exponential function is exponential function, we can just use output of `counts` for the local derivative and `dcounts` as one from the chain rule: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dnorm_logits\"\n",
    "dnorm_logits = counts * dcounts\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dlogits`, `dlogit_maxes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have a similar case where `logits` flows through two nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dlogits` (first node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "norm_logits = logits - logit_maxes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape, logits.shape, logit_maxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the shapes are the same for `norm_logits` and `logits` (`logit_maxes` turns to zero) so we have just take `dnorm_logits` from the chain rule and clone it via `clone()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dlogits\" (Node 1)\n",
    "dlogits = dnorm_logits.clone()\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we have actually obtained the approximate equality. However, we are not done yet, since we still need to consider the second branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dlogit_maxes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "norm_logits = logits - logit_maxes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_logits.shape, logits.shape, logit_maxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a similar situation where `torch` implements broadcasting: elements of `logit_maxes` are replicated across the columns of `logits`. Hence, as seen above, we can just sum `dnorm_logits` across the columns and put a negative sign: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dlogit_maxes\"\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dlogits` (second node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 27]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_maxes.shape, logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again use a simpler example to understand what such an operation entails:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$A^{max} = \\begin{bmatrix}\n",
    "\\max{a_{11}, a_{12}, a_{13}}\\\\\n",
    "\\max{a_{21}, a_{22}, a_{23}}\\\\\n",
    "\\max{a_{31}, a_{32}, a_{33}}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically take the maximum value in each row selecting from the values across the columns. In our case matrix $A$ (`logits`) looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5027e-01,  9.3988e-01, -4.1278e-01,  3.9874e-01, -4.9741e-01,\n",
       "          9.0140e-01, -3.1819e-01,  1.5104e-01, -4.0815e-01,  4.5677e-02,\n",
       "          2.0063e-01,  1.4772e-01,  1.9711e-01, -9.7188e-02,  1.1970e-01,\n",
       "         -7.8213e-01, -1.2028e+00, -4.5766e-01, -7.3102e-01,  5.3342e-01,\n",
       "          4.6820e-01, -4.1344e-01, -1.4548e-01,  7.5842e-01,  5.2346e-01,\n",
       "         -1.4266e-01, -3.7225e-01],\n",
       "        [ 3.2419e-01,  2.6773e-01,  9.1420e-01,  2.1835e-01, -1.6888e-01,\n",
       "         -2.6658e-01, -7.9142e-01,  1.1045e-01, -6.8897e-01, -3.8361e-01,\n",
       "          2.4757e-01,  1.1067e-01,  1.4007e-01, -3.6754e-01,  1.8753e-01,\n",
       "          6.8173e-02, -4.1529e-01, -7.1075e-01, -7.1830e-01, -8.6824e-02,\n",
       "         -7.0208e-01, -6.0863e-01, -9.8831e-01,  4.3196e-01, -5.6090e-01,\n",
       "         -5.9323e-02, -4.1684e-01],\n",
       "        [-4.5637e-01, -4.3566e-01, -7.1870e-01, -1.0700e+00, -4.4410e-01,\n",
       "          2.2059e-01,  5.4951e-01,  6.8897e-01,  6.0356e-01, -9.3277e-02,\n",
       "         -4.4054e-01,  3.1011e-02,  2.3304e-01,  3.1389e-01, -3.5934e-01,\n",
       "         -1.8052e-01, -8.5588e-01,  5.7183e-02, -1.6634e-01,  1.2537e+00,\n",
       "          6.8303e-01,  1.0219e-01,  2.7677e-01,  8.9045e-02, -9.3242e-04,\n",
       "         -4.7939e-01, -1.4358e-01],\n",
       "        [-1.5330e-02, -4.0007e-01,  2.9066e-01,  3.0778e-01,  4.6699e-01,\n",
       "         -2.8951e-01,  2.3763e-01,  1.6945e-01,  2.8036e-01, -5.4698e-01,\n",
       "          9.4109e-02, -2.8009e-01, -6.2999e-02,  7.3426e-02,  5.0649e-01,\n",
       "          7.1775e-01, -3.9473e-01, -2.6940e-01, -7.8884e-01,  3.5513e-01,\n",
       "         -6.2288e-01, -4.7351e-01,  1.6970e-01, -5.5073e-01, -3.8072e-01,\n",
       "          2.9227e-01,  2.0277e-01],\n",
       "        [-5.6554e-01, -7.3212e-01, -2.3668e-01, -3.7256e-01, -6.5474e-01,\n",
       "          2.8673e-01,  9.6836e-02, -4.5545e-01,  5.7541e-01,  1.9565e-01,\n",
       "          9.9467e-02, -6.9200e-01,  1.7509e-01, -3.3317e-01,  3.1071e-01,\n",
       "          1.7236e+00, -5.3379e-01,  5.0767e-01,  4.4063e-01,  6.6530e-01,\n",
       "          3.1921e-01, -7.8815e-01, -3.2897e-01, -7.6513e-01,  1.4136e-01,\n",
       "         -3.1705e-01,  3.6678e-01],\n",
       "        [ 9.1809e-02,  2.5506e-02,  6.3407e-01,  5.6357e-01,  6.4982e-01,\n",
       "         -3.1944e-01, -5.1614e-02,  4.8609e-01, -2.7196e-01, -7.0835e-01,\n",
       "          3.1679e-01, -5.9651e-01, -1.7487e-01,  2.3414e-01,  6.7568e-01,\n",
       "          5.0941e-01, -6.8085e-01, -3.6446e-01, -1.1501e+00, -4.1933e-02,\n",
       "         -3.0888e-02, -1.4960e-01,  3.1032e-01, -5.9341e-01, -6.6464e-01,\n",
       "          8.6121e-01,  2.6192e-01],\n",
       "        [ 4.7407e-01, -1.2373e+00,  5.8381e-01, -9.0873e-01, -2.8967e-01,\n",
       "         -6.8876e-01,  6.0091e-01,  4.7685e-01,  3.8397e-01,  1.8135e-01,\n",
       "         -6.5565e-01,  3.3892e-02, -3.0487e-01, -7.2402e-01, -8.8828e-01,\n",
       "          6.6685e-01,  7.0660e-01,  2.2181e-01, -1.2878e+00, -1.2382e-01,\n",
       "         -2.8903e-01,  2.7454e-01,  2.1954e-01, -5.4676e-01, -7.7745e-02,\n",
       "         -4.3057e-01, -2.4126e-01],\n",
       "        [ 4.6248e-01,  5.4923e-02, -1.3059e-01,  1.3514e+00, -7.1259e-02,\n",
       "          5.7577e-01,  8.4207e-02,  1.0949e+00,  3.1704e-01, -1.2085e+00,\n",
       "         -6.9639e-02, -6.7060e-01, -3.3357e-01,  6.9632e-01, -4.6731e-01,\n",
       "         -6.2250e-01, -7.0108e-01, -6.6517e-01, -6.6877e-01,  1.1841e+00,\n",
       "          2.5290e-01,  4.8613e-01, -4.1397e-02,  1.7982e-01, -2.6207e-01,\n",
       "          6.6934e-01,  2.0371e-01],\n",
       "        [ 4.5703e-01, -1.6618e-01, -5.8906e-01,  1.3423e-01, -1.0224e+00,\n",
       "          2.2696e-01,  5.9296e-01,  1.1333e+00, -5.6699e-02, -4.2001e-01,\n",
       "         -4.0780e-02, -3.4957e-03,  2.1926e-01, -2.2521e-01, -1.0563e+00,\n",
       "         -5.1454e-01, -4.5053e-01,  2.3631e-01, -3.8395e-01,  1.1245e+00,\n",
       "          1.4286e-01,  9.2035e-01,  3.2815e-01,  3.6492e-01,  1.8036e-01,\n",
       "         -7.1131e-01,  2.1897e-02],\n",
       "        [-4.8312e-02, -8.7067e-01,  7.2292e-02, -6.0816e-01, -7.6517e-02,\n",
       "          1.2662e-01,  6.2216e-02,  1.0562e-02,  1.0630e+00,  6.7593e-01,\n",
       "         -7.8193e-01,  2.1883e-01,  1.7372e-01,  2.2817e-01, -2.4887e-01,\n",
       "         -7.1907e-04,  6.5628e-01,  4.1098e-01,  2.4212e-01, -2.1510e-01,\n",
       "          7.7388e-01, -2.6817e-01,  7.3946e-01, -4.5973e-01, -1.2644e-01,\n",
       "          1.2182e-01, -4.8879e-01],\n",
       "        [-4.8510e-01, -1.3084e+00,  4.5013e-01,  4.5789e-01,  1.7641e-01,\n",
       "          2.8938e-01, -3.7820e-01, -1.3158e+00,  3.3479e-01,  5.8673e-01,\n",
       "         -4.3109e-02, -3.4037e-01,  7.2696e-01,  9.4658e-02,  4.5840e-01,\n",
       "          1.7201e+00,  6.7589e-01,  8.5706e-01,  1.0813e+00,  3.9525e-01,\n",
       "          5.4520e-01, -9.2621e-01, -7.3201e-01, -1.3341e-01,  2.3966e-01,\n",
       "          6.1712e-02, -6.1878e-02],\n",
       "        [-9.6899e-02,  1.1782e-01,  2.8779e-02,  1.9321e+00, -6.8798e-01,\n",
       "          7.2593e-01,  1.0039e+00,  6.1574e-01,  5.2620e-01,  2.1296e-01,\n",
       "         -5.2683e-01, -2.9636e-01, -1.9783e-01,  4.3734e-02, -1.3233e+00,\n",
       "         -1.1602e+00, -1.2246e-01,  4.4441e-01, -5.4318e-01,  7.5822e-01,\n",
       "          7.6784e-01,  2.5851e-01, -3.2629e-01,  4.1256e-01,  4.9466e-01,\n",
       "          5.2091e-01, -4.5721e-01],\n",
       "        [-4.6094e-01, -9.1471e-01,  4.9342e-01, -8.5401e-01, -2.5209e-01,\n",
       "         -6.8057e-01,  8.8213e-01,  2.6746e-01,  5.7224e-01,  4.2666e-01,\n",
       "         -6.3813e-01,  1.3901e-01,  1.5729e-01, -1.6171e-01, -7.5838e-01,\n",
       "          2.7970e-01,  8.1075e-01,  3.7322e-01,  1.8258e-01, -2.5285e-01,\n",
       "          3.8588e-01,  2.0969e-01,  1.0995e+00, -1.0966e+00, -5.0256e-01,\n",
       "         -2.1904e-01, -1.6233e-01],\n",
       "        [-2.3539e-01, -2.6905e-01,  5.0136e-01, -5.8816e-01, -7.0349e-01,\n",
       "          7.7564e-01, -2.6305e-01, -7.5458e-01, -1.6485e-01,  3.9981e-01,\n",
       "         -4.2695e-02,  2.5036e-02,  5.9786e-01,  5.9750e-02,  4.4049e-01,\n",
       "          3.3244e-01, -1.6348e-01,  4.1967e-02,  7.7145e-01, -8.6113e-01,\n",
       "          6.6591e-01, -8.3285e-01, -6.4097e-01,  3.7408e-02, -9.2743e-02,\n",
       "          1.3846e-01,  6.0267e-02],\n",
       "        [ 3.6652e-01, -4.4422e-01, -1.0741e-01,  4.1765e-01, -6.8964e-01,\n",
       "         -1.5080e-03, -1.0283e-01,  6.4046e-01,  5.6517e-01,  3.7941e-01,\n",
       "         -3.9101e-02,  2.6046e-01,  4.3195e-01,  1.0572e-01, -2.4016e-01,\n",
       "         -2.6983e-01, -4.4606e-01,  5.2820e-01, -2.6346e-01,  4.1585e-01,\n",
       "          4.9435e-01,  3.6060e-01, -2.2646e-01, -3.3640e-02, -6.2828e-01,\n",
       "         -4.4034e-01, -3.5810e-01],\n",
       "        [ 1.6931e-01,  3.6832e-01, -1.0851e-01, -7.5266e-01, -6.8221e-01,\n",
       "          7.4703e-01, -2.9703e-01, -3.9109e-01, -2.3021e-01,  3.7478e-01,\n",
       "         -3.9553e-02,  2.4916e-01,  4.9718e-01, -8.3431e-01, -2.4591e-01,\n",
       "         -7.0404e-01,  7.3973e-02, -5.8337e-01,  3.6574e-01, -7.4207e-01,\n",
       "          6.3137e-02, -4.3402e-01, -1.2156e-01,  5.6131e-01,  3.0228e-01,\n",
       "         -2.4084e-01, -5.2683e-01],\n",
       "        [-9.6594e-02, -4.0395e-01,  1.0149e+00,  2.0247e-01,  6.2164e-01,\n",
       "          4.0705e-01, -7.3815e-01, -1.7081e-01, -5.5338e-01, -4.7827e-01,\n",
       "          4.6228e-01, -4.0981e-01,  2.1742e-01,  4.7753e-01,  7.1465e-01,\n",
       "          7.3833e-01, -8.0222e-01, -5.4023e-01, -4.3400e-01,  2.8874e-01,\n",
       "         -1.6926e-01, -7.9814e-01, -1.2731e+00,  3.3417e-01, -4.3747e-01,\n",
       "          3.1054e-01,  5.8081e-01],\n",
       "        [ 4.1473e-01,  7.3136e-01,  3.1500e-01,  3.9071e-01,  4.2763e-02,\n",
       "         -1.3048e-01, -5.0295e-01,  6.8432e-01, -3.5238e-01, -6.7834e-01,\n",
       "          6.5809e-01, -9.8765e-01, -4.5438e-01, -6.1229e-01,  2.0660e-01,\n",
       "         -1.9376e-01, -8.9721e-01, -1.0403e-01, -8.2155e-01,  3.8776e-01,\n",
       "         -5.7094e-01,  2.4560e-01, -2.5052e-01, -6.4522e-01, -4.1662e-01,\n",
       "          1.8459e-01,  4.9867e-01],\n",
       "        [-4.6094e-01, -9.1471e-01,  4.9342e-01, -8.5401e-01, -2.5209e-01,\n",
       "         -6.8057e-01,  8.8213e-01,  2.6746e-01,  5.7224e-01,  4.2666e-01,\n",
       "         -6.3813e-01,  1.3901e-01,  1.5729e-01, -1.6171e-01, -7.5838e-01,\n",
       "          2.7970e-01,  8.1075e-01,  3.7322e-01,  1.8258e-01, -2.5285e-01,\n",
       "          3.8588e-01,  2.0969e-01,  1.0995e+00, -1.0966e+00, -5.0256e-01,\n",
       "         -2.1904e-01, -1.6233e-01],\n",
       "        [-4.0776e-01, -6.4457e-01,  6.9110e-03, -8.8488e-01, -6.5536e-02,\n",
       "         -1.0649e-01,  6.5156e-01,  4.2077e-01, -2.6885e-02, -7.7908e-02,\n",
       "         -1.9671e-01,  9.4441e-02,  4.2797e-01,  3.3716e-01, -9.4137e-01,\n",
       "         -3.4677e-01, -2.2601e-01, -5.7413e-01, -2.5428e-01,  8.6167e-01,\n",
       "          3.1022e-01, -6.9789e-02,  1.5322e-01,  2.1042e-01, -2.5420e-01,\n",
       "         -4.7754e-01, -3.2602e-01],\n",
       "        [ 4.8036e-01, -9.6019e-01, -5.9232e-01, -7.6228e-03, -1.5586e-01,\n",
       "          4.5676e-02, -1.5948e-01,  2.1056e-01,  4.8541e-01, -4.3482e-01,\n",
       "          6.4253e-01,  4.1553e-02,  5.3899e-01, -5.3564e-01,  2.0757e-01,\n",
       "          9.0423e-01, -7.0229e-01, -4.1360e-01,  4.6196e-02,  5.1284e-01,\n",
       "         -6.1710e-01, -2.0734e-01, -2.5588e-01, -3.3943e-01,  1.3438e-02,\n",
       "          5.1321e-01,  1.9905e-01],\n",
       "        [ 5.1867e-01,  7.0681e-01,  5.4093e-01,  3.0313e-01, -2.5110e-01,\n",
       "          4.9290e-01, -1.4952e-01,  3.7463e-01, -3.5818e-01, -5.4220e-01,\n",
       "          5.5157e-01, -1.3467e-01, -2.7860e-01, -9.1366e-02,  2.3241e-01,\n",
       "          4.7422e-01, -5.4273e-01,  2.8666e-01, -5.1847e-01,  9.3282e-01,\n",
       "         -8.3524e-01, -7.6538e-02, -7.6548e-01, -1.1741e-01, -6.2918e-02,\n",
       "         -8.7660e-02,  1.4257e-01],\n",
       "        [-4.6094e-01, -9.1471e-01,  4.9342e-01, -8.5401e-01, -2.5209e-01,\n",
       "         -6.8057e-01,  8.8213e-01,  2.6746e-01,  5.7224e-01,  4.2666e-01,\n",
       "         -6.3813e-01,  1.3901e-01,  1.5729e-01, -1.6171e-01, -7.5838e-01,\n",
       "          2.7970e-01,  8.1075e-01,  3.7322e-01,  1.8258e-01, -2.5285e-01,\n",
       "          3.8588e-01,  2.0969e-01,  1.0995e+00, -1.0966e+00, -5.0256e-01,\n",
       "         -2.1904e-01, -1.6233e-01],\n",
       "        [ 4.2530e-01, -6.6016e-01, -1.3391e-01, -1.0968e+00,  4.6082e-01,\n",
       "         -3.1804e-01,  6.0518e-01,  4.0122e-02, -2.8301e-01,  2.5648e-01,\n",
       "          8.4042e-04,  2.5101e-01,  1.7663e-02, -5.2235e-02,  1.0288e-01,\n",
       "          4.4163e-01,  7.4119e-01, -3.6771e-01,  9.8506e-03, -7.7911e-01,\n",
       "         -4.9667e-01, -4.6859e-01,  1.2289e+00, -5.6850e-01,  4.0153e-01,\n",
       "         -6.1019e-02, -5.8994e-01],\n",
       "        [ 8.1103e-02,  8.6377e-01, -3.1898e-01,  3.0732e-01, -2.8447e-01,\n",
       "          6.1377e-01, -3.6283e-01, -1.2232e-01, -1.9010e-01, -1.2840e-02,\n",
       "          1.0915e+00, -6.2870e-01, -1.0926e-01, -1.0544e+00,  1.3694e-01,\n",
       "         -4.8406e-01, -6.8363e-01, -6.2521e-01, -1.0546e+00, -2.0304e-01,\n",
       "         -8.6123e-01, -3.6730e-01, -1.0917e+00,  1.4748e+00,  3.7264e-01,\n",
       "         -1.8144e-01,  4.2676e-01],\n",
       "        [ 7.5489e-01, -1.7103e-01, -6.6508e-01, -5.5612e-01, -9.1332e-02,\n",
       "          9.4456e-01,  2.9724e-01,  8.6871e-02, -6.9115e-02,  1.0510e-01,\n",
       "          1.9760e-02,  5.8941e-01,  5.5388e-01,  7.6123e-02, -3.6964e-01,\n",
       "         -8.1073e-01,  4.7431e-02, -3.9312e-01,  5.2523e-01, -7.8476e-01,\n",
       "         -1.3267e-01, -3.7450e-01,  8.6671e-01,  4.7212e-01,  6.8369e-01,\n",
       "          3.6450e-01, -9.6098e-01],\n",
       "        [ 1.8465e-01, -4.7090e-01, -4.9137e-01, -1.3697e+00,  6.9824e-02,\n",
       "         -6.4577e-01,  7.8787e-01,  1.7918e-01, -1.7824e-01,  3.3215e-01,\n",
       "         -7.1623e-01,  2.8620e-01,  4.5502e-01, -2.4641e-01, -3.9708e-01,\n",
       "         -6.3948e-01,  3.5530e-01, -3.1728e-01, -3.3732e-01, -1.6342e-01,\n",
       "         -4.6990e-01, -7.2211e-02,  1.2910e+00, -7.0758e-01, -1.2399e-01,\n",
       "         -2.9757e-01, -1.2375e+00],\n",
       "        [-2.2413e-01, -2.7783e-01,  4.1649e-01, -6.0491e-01, -1.1574e-01,\n",
       "         -4.1540e-01,  1.6301e-02, -1.2658e-01,  1.7819e-01,  3.7990e-01,\n",
       "         -4.4649e-01, -1.9997e-01,  6.6445e-02, -3.4833e-02,  2.4677e-01,\n",
       "          6.4869e-01,  2.2819e-01,  3.0148e-01,  3.1562e-01, -7.0204e-01,\n",
       "          9.3537e-01, -8.5310e-01,  3.2299e-01, -1.0894e+00, -8.2845e-01,\n",
       "          1.2405e-03,  2.1494e-01],\n",
       "        [ 8.2288e-01,  1.1554e-01, -6.1229e-01, -5.2827e-01,  1.0295e-01,\n",
       "          6.1961e-01,  3.1440e-01,  3.4973e-01, -1.0901e+00, -1.3074e-01,\n",
       "          4.0937e-01,  2.2077e-02, -2.3543e-01, -3.7054e-01, -1.0707e-01,\n",
       "          1.0083e-01,  9.2176e-01, -1.5715e-01,  6.4160e-01, -2.9283e-01,\n",
       "          7.7237e-02, -8.8101e-02,  4.3005e-01,  5.8395e-01,  7.7563e-01,\n",
       "         -9.5675e-01, -4.3797e-01],\n",
       "        [-3.3785e-01, -7.2918e-01,  9.6906e-02, -1.5622e-01,  3.7752e-01,\n",
       "         -4.9211e-01,  6.6300e-01,  5.0337e-02,  7.6191e-01,  9.1412e-03,\n",
       "         -1.0282e-01,  1.0080e-01, -9.5352e-02,  2.0533e-02, -8.7430e-02,\n",
       "          8.2383e-01, -1.3833e-01, -1.0921e-01,  3.8497e-02, -8.0202e-02,\n",
       "          3.1515e-01, -2.9627e-01,  5.6900e-01, -8.1481e-01, -1.0162e-01,\n",
       "          4.5385e-01,  1.3247e-01],\n",
       "        [ 3.5279e-01,  4.7411e-01, -9.7932e-02,  6.9921e-01,  4.4218e-01,\n",
       "          4.3848e-01,  2.3268e-01,  1.5821e-01, -3.1018e-03, -2.7597e-01,\n",
       "          2.8585e-01, -1.9992e-01, -4.7651e-01, -1.3642e-01, -9.8069e-01,\n",
       "          2.9602e-01,  1.8717e-01,  1.1708e-01,  3.4567e-01,  5.6482e-01,\n",
       "          6.5709e-01, -1.1035e-01, -2.2516e-01,  9.2068e-01,  9.5191e-01,\n",
       "         -3.3009e-01, -5.2042e-02],\n",
       "        [ 3.3613e-01, -4.1946e-01,  8.2144e-01, -6.1346e-01,  7.6982e-01,\n",
       "         -1.1461e-01, -6.4760e-01, -2.0513e-01, -7.2610e-01,  4.9244e-01,\n",
       "         -1.9689e-01, -3.2769e-02,  1.2143e-02,  8.1253e-01,  2.0543e-01,\n",
       "          2.1010e-01,  4.5782e-01,  2.1387e-01,  2.3783e-01, -8.7458e-01,\n",
       "          6.1116e-01, -1.1820e+00,  6.0905e-01, -3.7325e-01, -6.1490e-01,\n",
       "          1.5791e-01, -4.6403e-01]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation of taking the maximum value across columns is equivalent to the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9399, 0.9142, 1.2537, 0.7177, 1.7236, 0.8612, 0.7066, 1.3514, 1.1333,\n",
       "        1.0630, 1.7201, 1.9321, 1.0995, 0.7756, 0.6405, 0.7470, 1.0149, 0.7314,\n",
       "        1.0995, 0.8617, 0.9042, 0.9328, 1.0995, 1.2289, 1.4748, 0.9446, 1.2910,\n",
       "        0.9354, 0.9218, 0.8238, 0.9519, 0.8214], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 1,  2, 19, 15, 15, 25, 16,  3,  7,  8, 15,  3, 22,  5,  7,  5,  2,  1,\n",
       "        22, 19, 15, 19, 22, 22, 23,  5, 22, 20, 16, 15, 24,  2]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to calculating the maximum values, `max` method also returns the index where this value has been found. We can use this information to compose an array returning one for the maximum value. Basically, ones will denote the place where the gradient is not zero. There are many ways to do this but we will use `one_hot` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Accumulating \"dloss/dlogits\" from Node 2\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the linear layer 2. Here is the expression that we need to backrpropagate through:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "logits = h @ W2 + b2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64, 27]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, h.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to easier define what a derivate looks like with respect to all right-hand side elements of the above expression, let's consider a simpler case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$D = A \\times B + C$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "d_{11} & d_{12}\\\\\n",
    "d_{21} & d_{22}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "b_{11} & b_{12}\\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "c_{1} & c_{2}\\\\\n",
    "c_{1} & c_{2}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have the matrix representation of the equation where matrix $C$ (that is `b2`) is getting broadcast across all rows of $A \\times B$ (since `b2` is not a column but row vector). Now we will try to derive the derivative with respect to each elements of $A$, $B$ and $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will find the gradient of `loss` with respect to `h`. In other words, in our toy example, we will be looking for the derivative with respect to elements of $A$. Let's write down what each element of $D$ looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_{11} = a_{11} b_{11} + a_{12} b_{21} + c_1$$\n",
    "$$d_{12} = a_{11} b_{12} + a_{12} b_{22} + c_2$$\n",
    "$$d_{21} = a_{21} b_{11} + a_{22} b_{21} + c_1$$\n",
    "$$d_{22} = a_{21} b_{12} + a_{22} b_{22} + c_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's start calculating the derivatives of loss function $L$ using the chain rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{L}}{\\partial{a_{11}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} \\frac{\\partial{d_{11}}}{\\partial{a_{11}}} + \\frac{\\partial{L}}{\\partial{d_{12}}} \\frac{\\partial{d_{12}}}{\\partial{a_{11}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} b_{11} + \\frac{\\partial{L}}{\\partial{d_{12}}} b_{12}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{a_{12}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} \\frac{\\partial{d_{11}}}{\\partial{a_{12}}} + \\frac{\\partial{L}}{\\partial{d_{12}}} \\frac{\\partial{d_{12}}}{\\partial{a_{12}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} b_{21} + \\frac{\\partial{L}}{\\partial{d_{12}}} b_{22}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{a_{21}}} = \\frac{\\partial{L}}{\\partial{d_{21}}} \\frac{\\partial{d_{21}}}{\\partial{a_{21}}} + \\frac{\\partial{L}}{\\partial{d_{22}}} \\frac{\\partial{d_{22}}}{\\partial{a_{21}}} = \\frac{\\partial{L}}{\\partial{d_{21}}} b_{11} + \\frac{\\partial{L}}{\\partial{d_{22}}} b_{12}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{a_{22}}} = \\frac{\\partial{L}}{\\partial{d_{21}}} \\frac{\\partial{d_{21}}}{\\partial{a_{22}}} + \\frac{\\partial{L}}{\\partial{d_{22}}} \\frac{\\partial{d_{22}}}{\\partial{a_{22}}} = \\frac{\\partial{L}}{\\partial{d_{21}}} b_{21} + \\frac{\\partial{L}}{\\partial{d_{22}}} b_{22}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can actually be seen that representing the gradients in a matrix form allows us to rewrite everything down as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{a_{11}}} & \\frac{\\partial{L}}{\\partial{a_{12}}}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{a_{21}}} & \\frac{\\partial{L}}{\\partial{a_{22}}}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{d_{11}}} & \\frac{\\partial{L}}{\\partial{d_{12}}}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{d_{21}}} & \\frac{\\partial{L}}{\\partial{d_{22}}}\\\\\n",
    "\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "b_{11} & b_{21}\\\\\n",
    "b_{12} & b_{22}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{a}} = \\frac{\\partial{L}}{\\partial{d}} \\times b^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this formula to compute the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dh\"\n",
    "dh = dlogits @ W2.T\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('h', dh, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dW2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the exact same with elements of $B$ matrix (gradient of `loss` with respect to `W2`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_{11} = a_{11} b_{11} + a_{12} b_{21} + c_1$$\n",
    "$$d_{12} = a_{11} b_{12} + a_{12} b_{22} + c_2$$\n",
    "$$d_{21} = a_{21} b_{11} + a_{22} b_{21} + c_1$$\n",
    "$$d_{22} = a_{21} b_{12} + a_{22} b_{22} + c_2$$\n",
    "\n",
    "$$\\Downarrow$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{L}}{\\partial{b_{11}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} \\frac{\\partial{d_{11}}}{\\partial{b_{11}}} + \\frac{\\partial{L}}{\\partial{d_{21}}} \\frac{\\partial{d_{21}}}{\\partial{b_{11}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} a_{11} + \\frac{\\partial{L}}{\\partial{d_{21}}} a_{21}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b_{12}}} = \\frac{\\partial{L}}{\\partial{d_{12}}} \\frac{\\partial{d_{12}}}{\\partial{b_{12}}} + \\frac{\\partial{L}}{\\partial{d_{22}}} \\frac{\\partial{d_{22}}}{\\partial{b_{12}}} = \\frac{\\partial{L}}{\\partial{d_{12}}} a_{11} + \\frac{\\partial{L}}{\\partial{d_{22}}} a_{21}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b_{21}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} \\frac{\\partial{d_{11}}}{\\partial{b_{21}}} + \\frac{\\partial{L}}{\\partial{d_{21}}} \\frac{\\partial{d_{21}}}{\\partial{b_{21}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} a_{12} + \\frac{\\partial{L}}{\\partial{d_{21}}} a_{22}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b_{22}}} = \\frac{\\partial{L}}{\\partial{d_{12}}} \\frac{\\partial{d_{12}}}{\\partial{b_{22}}} + \\frac{\\partial{L}}{\\partial{d_{22}}} \\frac{\\partial{d_{22}}}{\\partial{b_{22}}} = \\frac{\\partial{L}}{\\partial{d_{12}}} a_{12} + \\frac{\\partial{L}}{\\partial{d_{22}}} a_{22}$$\n",
    "\n",
    "$$\\Downarrow$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{b_{11}}} & \\frac{\\partial{L}}{\\partial{b_{12}}}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{b_{21}}} & \\frac{\\partial{L}}{\\partial{b_{22}}}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a_{11} & a_{21}\\\\\n",
    "a_{12} & a_{22}\\\\\n",
    "\\end{bmatrix} \\times \\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{d_{11}}} & \\frac{\\partial{L}}{\\partial{d_{12}}}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{d_{21}}} & \\frac{\\partial{L}}{\\partial{d_{22}}}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = a^{T} \\times \\frac{\\partial{L}}{\\partial{d}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dW2\"\n",
    "dW2 = h.T @ dlogits\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('W2', dW2, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `db2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can do that for the bias term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_{11} = a_{11} b_{11} + a_{12} b_{21} + c_1$$\n",
    "$$d_{12} = a_{11} b_{12} + a_{12} b_{22} + c_2$$\n",
    "$$d_{21} = a_{21} b_{11} + a_{22} b_{21} + c_1$$\n",
    "$$d_{22} = a_{21} b_{12} + a_{22} b_{22} + c_2$$\n",
    "\n",
    "$$\\Downarrow$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{L}}{\\partial{c_{1}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} \\frac{\\partial{d_{11}}}{\\partial{c_{1}}} + \\frac{\\partial{L}}{\\partial{d_{21}}} \\frac{\\partial{d_{21}}}{\\partial{c_{1}}} = \\frac{\\partial{L}}{\\partial{d_{11}}} + \\frac{\\partial{L}}{\\partial{d_{21}}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{c_{2}}} = \\frac{\\partial{L}}{\\partial{d_{12}}} \\frac{\\partial{d_{12}}}{\\partial{c_{2}}} + \\frac{\\partial{L}}{\\partial{d_{22}}} \\frac{\\partial{d_{22}}}{\\partial{c_{2}}} = \\frac{\\partial{L}}{\\partial{d_{12}}} + \\frac{\\partial{L}}{\\partial{d_{22}}}$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{d}} = \\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{d_{11}}} & \\frac{\\partial{L}}{\\partial{d_{12}}}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{d_{21}}} & \\frac{\\partial{L}}{\\partial{d_{22}}}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{c}} = \\begin{bmatrix}\n",
    "\\frac{\\partial{L}}{\\partial{d_{11}}} + \\frac{\\partial{L}}{\\partial{d_{21}}}\\\\\n",
    "\\frac{\\partial{L}}{\\partial{d_{12}}} + \\frac{\\partial{L}}{\\partial{d_{22}}}\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can seen here that the derivatives with respect to $c$ just represent the elements of loss function gradient with respect to logits where these are summed vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/db2\"\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "h = torch.tanh(hpreact)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move to the Tanh layer. The tanh function looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of such a function takes a complex form which is not really efficient for the backpropagation. However, it is possible to rewrite the derivative as follows:\n",
    "\n",
    "$$\\frac{\\partial{\\tanh(x)}}{\\partial{x}} = \\frac{(e^{x} + e^{-x}) (e^{x} + e^{-x}) - (e^{x} - e^{-x}) (e^{x} - e^{-x})}{(e^{x} + e^{-x})^2}$$\n",
    "\n",
    "$$\\frac{\\partial{\\tanh(x)}}{\\partial{x}} = \\frac{(e^{x} + e^{-x})^2 - (e^{x} - e^{-x})^2}{(e^{x} + e^{-x})^2}$$\n",
    "\n",
    "$$\\frac{\\partial{\\tanh(x)}}{\\partial{x}} = 1 - \\frac{(e^{x} - e^{-x})^2}{(e^{x} + e^{-x})^2}$$\n",
    "\n",
    "$$\\frac{\\partial{\\tanh(x)}}{\\partial{x}} = 1 - \\left(\\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\\right)^2$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$\\frac{\\partial{\\tanh(x)}}{\\partial{x}} = 1 - \\tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we can already use the output of the hyperbolic tangent function to easily compute the derivative. `h` already stores the result of tanh function so we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dhpreact\"\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached the batch normalization layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbngain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([1, 64]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bngain` is replicated across the rows of `bnraw` so we have to consider the vertical sums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbngain\"\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bngain', dbngain, bngain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbnraw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([1, 64]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbnraw\"\n",
    "dbnraw = bngain * dhpreact\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bnraw', dbnraw, bnraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbnbias`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([1, 64]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbnbias\"\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bnbias', dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbndiff` (first node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to be careful, since `bndiff` occurs in two nodes. Let's consider the first one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnraw.shape, bndiff.shape, bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff          | exact: False | approximate: False | maxdiff: 0.001157917082309723\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbndiff\" (Node 1)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bndiff', dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbnvar_inv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bnraw = bndiff * bnvar_inv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnraw.shape, bndiff.shape, bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbnvar_inv\"\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbnvar`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnvar_inv.shape, bnvar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbnvar\"\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bnvar_inv', dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbndiff2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnvar.shape, bndiff.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a toy example to depict what the above operation achieves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$A^{sum} = \\frac{1}{2} \\begin{bmatrix}\n",
    "a_{11} + a_{21} + a_{31}\\\\\n",
    "a_{12} + a_{22} + a_{32}\\\\\n",
    "a_{13} + a_{23} + a_{33}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be notices, we are summing up the elements of $A$ across the rows and apply the scaling factor (Bessel correction) afterwards. The derivative of this is just a matrix of ones multiplied by chain rule derivative with scaling factor accounted for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbndiff2\"\n",
    "dbndiff2 = (1.0 / (n - 1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bndiff2', dbndiff2, bndiff2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbndiff` (second node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached the second node where `bndiff` occurs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bndiff2 = bndiff**2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff2.shape, bndiff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Accumulating \"dloss/dbndiff\" from Node 2\n",
    "dbndiff += 2.0 * bndiff * dbndiff2\n",
    "\n",
    "# Comapring the gradient computed\n",
    "cmp('bndiff', dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dhprebn` (first node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dhprebn` also flows through two channels. The first node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bndiff = hprebn - bnmeani\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff.shape, hprebn.shape, bnmeani.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: False | maxdiff: 0.0011137898545712233\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dhprebn\" (Node 1)\n",
    "dhprebn = dbndiff.clone()\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dbnmeani`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bndiff = hprebn - bnmeani\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff.shape, hprebn.shape, bnmeani.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dbnmeani\"\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('bnmeani', dbnmeani, bnmeani)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dhprebn` (second node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the second node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmeani.shape, hprebn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simpler example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$A^{sum} = \\frac{1}{3} \\begin{bmatrix}\n",
    "a_{11} + a_{21} + a_{31}\\\\\n",
    "a_{12} + a_{22} + a_{32}\\\\\n",
    "a_{13} + a_{23} + a_{33}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Accumulating \"dloss/dhprebn\" from Node 2\n",
    "dhprebn += (1.0 / n) * torch.ones_like(hprebn) * dbnmeani\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached another linear layer and we can apply the above computed formulas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "hprebn = embcat @ W1 + b1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dembcat\"\n",
    "dembcat = dhprebn @ W1.T\n",
    "# Computing \"dloss/dW1\"\n",
    "dW1 = embcat.T @ dhprebn\n",
    "# Computing \"dloss/db1\"\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "# Comparing the gradients computed\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next expression to get backpropagated through is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 30]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embcat.shape, emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this operation basically does is that we are just rearranging the elements of `emb` to obtain `embcat`. The derivative will just be the rearranged representation of `dembcat` from the chain rule in accordance with the dimensions of `emb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/demb\"\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "# Comparing the gradient computed\n",
    "cmp('emb', demb, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last operation to consider is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "emb = C[Xb]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 10]), torch.Size([27, 10]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, C.shape, Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  4],\n",
       "        [18, 14,  1],\n",
       "        [11,  5,  9],\n",
       "        [ 0,  0,  1],\n",
       "        [12, 15, 14]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing \"dloss/dC\"\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]\n",
    "        dC[ix] += demb[k, j]\n",
    "\n",
    "# Verifying the gradient computed\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of all gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to verify once again the correctness of all gradients we have manually computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
